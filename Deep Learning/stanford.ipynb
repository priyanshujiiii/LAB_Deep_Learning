{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "In today's lab, we will be working with tensorflow to see an implementation of a neural net (deep learning) in action.\n",
    "\n",
    "We will be using an ipython notebook. If you are familiar with ipython notebooks, you can skip to the \"Tensorflow\" section. Otherwise, keep reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ipython Notebooks\n",
    "Ipython notebooks are an interactive environment for running code and viewing images. Code is written in cells like the one below, and in order to run the code, you must select the cell by clicking on it, and then press enter while holding down shift. Try this with the below cell to see the first 10 primes printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")\n",
    "print(\"The first 10 primes are:\")\n",
    "primes = [2]\n",
    "num = 2\n",
    "while len(primes) < 10:\n",
    "    num_is_prime = True\n",
    "    for prime in primes:\n",
    "        if num % prime == 0:\n",
    "            num_is_prime = False\n",
    "    if num_is_prime:\n",
    "        primes.append(num)\n",
    "    num += 1\n",
    "print(primes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running multiple cells\n",
    "Whenever a cell is run in an ipython notebook, the variables it creates are saved and can be accessed from any other cell (it's like the interactive python shell). Try running the following two cells in sequence. This is important because the notebook is designed so that all the cells will be run in sequence. So, don't skip ahead to later cells without first running all the previous cells in order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_string = \"I ran these two cells in sequence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "Let's get started using tensorflow! First, import all the packages we will need: If you get a warning about the compiletime version of tensorflow not matching the runtime version, or deprecated support, just ignore it, everything still works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# logistic regression: http://web.stanford.edu/class/cs109/lectureHandouts/25%20LogisticRegression.pdf\n",
    "# gradient ascent: http://web.stanford.edu/class/cs109/lectureHandouts/22%20GradientAscent.pdf\n",
    "# thanks http://machinelearninguru.com/deep_learning/tensorflow/machine_learning_basics/logistic_regresstion/logistic_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "We will be working with the MNIST dataset, which is a dataset of hand-written digits. We will only work with the\n",
    "zeros and ones, so we can use logistic regression to classify whether an image is a 0 or a 1. Load the dataset and print out how many training and testing datapoints we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=True, one_hot=False)\n",
    "data={}\n",
    "data['train/image'] = mnist.train.images\n",
    "data['train/label'] = mnist.train.labels\n",
    "data['test/image'] = mnist.test.images\n",
    "data['test/label'] = mnist.test.labels\n",
    "# Get only the samples with zero and one label for training.\n",
    "index_list_train = []\n",
    "for sample_index in range(data['train/label'].shape[0]):\n",
    "    label = data['train/label'][sample_index]\n",
    "    if label == 1 or label == 0:\n",
    "        index_list_train.append(sample_index)\n",
    "# Reform the train data structure.\n",
    "data['train/image'] = mnist.train.images[index_list_train]\n",
    "data['train/label'] = mnist.train.labels[index_list_train]\n",
    "# Get only the samples with zero and one label for test set.\n",
    "index_list_test = []\n",
    "for sample_index in range(data['test/label'].shape[0]):\n",
    "    label = data['test/label'][sample_index]\n",
    "    if label == 1 or label == 0:\n",
    "        index_list_test.append(sample_index)\n",
    "# Reform the test data structure.\n",
    "data['test/image'] = mnist.test.images[index_list_test]\n",
    "data['test/label'] = mnist.test.labels[index_list_test]\n",
    "\n",
    "print(\"\\nNumber of training datapoints: %d\" % data['train/label'].shape[0])\n",
    "print(\"Number of testing datapoints: %d\" % data['test/label'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing MNIST examples\n",
    "Let's visualize a few example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's visualize a couple examples of mnist images:\"\"\"\n",
    "def gen_image(arr, ax):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    ax.imshow(two_d, cmap='gray')\n",
    "\n",
    "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, sharey=True, figsize=(8,1))\n",
    "gen_image(data['train/image'][0], ax1)\n",
    "gen_image(data['train/image'][1], ax2)\n",
    "gen_image(data['train/image'][2], ax3)\n",
    "gen_image(data['train/image'][3], ax4)\n",
    "gen_image(data['train/image'][4], ax5)\n",
    "print(\"Labels: %d, %d, %d, %d, %d\" % tuple(data['train/label'][:5].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the deep learning model (2 layer fully connected neural network):\n",
    "First, just set some parameters. These work well, but feel free to play around with them if you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 5\n",
    "training_epochs = 1000\n",
    "batch_size = 100 # the number of training images we use for each gradient ascent update\n",
    "display_step = 10\n",
    "n_hidden = 2 # the number of \"neurons\" in the first hidden layer of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to build the model. Tensorflow works by setting up a computation graph that input is then fed into. \n",
    "We first specify the inputs to the graph with tf.placeholder. Basically, x and y must be specified when we run the model.\n",
    "x will be a batch of input images, and y will be a batch of labels for the image (if necessary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 1]) # 0-1 digits recognition => 2 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to setup the parameters of the model. For now, just run this cell, and we'll go over what the variables are exactly later in the notebook. Note that 784 = 28*28 = the number of pixels in an mnist image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set model weights/parameters\n",
    "# note that 784 = 28*28 = the number of pixels in an mnist image.\n",
    "W_h = tf.get_variable(\"W_h\", shape=[784, n_hidden],\n",
    "       initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_h = tf.Variable(tf.zeros([n_hidden]))\n",
    "W_y = tf.get_variable(\"W_y\", shape=[n_hidden, 1],\n",
    "       initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_y = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the interesting part, constructing the model!\n",
    "\n",
    "\"a\" in the below cell is performing a bunch of different logistic regressions on the flattened input image, x. \n",
    "\n",
    "tf.matmul(x, W_h) does a matrix multiplication between x and W_h. This corresponds to $\\theta^T x$ in logistic regression. b_h is just $\\theta_0$. However, notice that the size of W_h, as specified in the above cell, is 784 x n_hidden. Basically, this means that we are performing n_hidden different logistic regressions on the input, all with their own $\\theta$ parameters. The same goes for b_h - there is a unique $\\theta_0$ for each logistic regression on the input data. \n",
    "\n",
    "Now, the key difference between \"deep learning\" or \"neural nets\" and simple logistic regression: we perform logistic regression AGAIN, on the outputs of the first logistic regression pass, to get our final output probabilities, prob_y1! \n",
    "\n",
    "To provide a little more detail for the interested: What we have done here is apply a linear transformation to the input image\n",
    "tf.matmul(x, W_h) + b_h\n",
    "mapping the input image to a n_hidden dimensional space. Then we apply some non-linear function to this linear transformation - in this case the sigmoid, but it can be many other things - tf.tanh, or tf.relu are popular choices - and then we just repeat the above process many times. Apply a linear transformation, and then a nonlinear function to the outputs. We only need two layers (one linear transformation, one nonlinear function, and then another linear transformation) to get a universal function approximator (which means the neural net can approximate any function to arbitrary precision, if n_hidden is large enough), but deeper neural nets with more layers allow this approximation to be better with smaller values of n_hidden. \n",
    "\n",
    "TODO add image to this cell of neural network, or go over on board in section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "\n",
    "a = tf.nn.sigmoid(tf.matmul(x, W_h) + b_h) # activate the hidden layer\n",
    "prob_y1 = tf.nn.sigmoid(tf.matmul(a, W_y) + b_y) # softmax with 2 dimensions is just a sigmoid\n",
    "prob_y0 = 1 - prob_y1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute loss\n",
    "Now that we have setup our model, for any given input images x and their classifications y, we can compute the negative log likelihood loss as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maximize log likelihood (aka minimize negative log likelihood) using cross entropy\n",
    "neg_LL = - tf.reduce_mean(y*tf.log(prob_y1)+(1-y)*tf.log(prob_y0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.log just takes the element-wise logarithm of the inputs.\n",
    "\n",
    "tf.reduce_mean takes the mean of all entries in an array (so this is actually -LL/N, where N is batch size)\n",
    "\n",
    "tensorflow supports broadcasting just like numpy. So 1 - A just replaces every entry $a_{ij}$ with $1 - a_{ij}$\n",
    "\n",
    "Can you recognize how the above cell computes negative log likelihood?\n",
    "\n",
    "$-LL = - \\sum_x \\log p(y = \\hat{y} | x)$\n",
    "\n",
    "$ = - \\sum_{x | \\hat{y}=1} p(y=1 | x) + \\sum_{x | \\hat{y}=0} p(y=0 | x) $\n",
    "\n",
    "$ = - \\sum_x \\left( (\\hat{y}) p(y=1|x) + (1-\\hat{y}) p(y=0 | x) \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient ascent / descent with tensorflow:\n",
    "Now, we're just going to add an operation to the graph that performs gradient descent for us (to minimize -LL)!\n",
    "The way this works is tensorflow computes the gradients of cost with respect the the four variables we specified, \n",
    "W_h, b_h, W_y, and b_y. Then, every time we evaluate the \"optimizer\" operation, it will perform a gradient update step for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Gradient Descent (NOTE: gradient ascent of log likelihood is the same as gradient descent of \n",
    "the negative log likelihood. Tensorflow has handy implementations of gradient descent, but not ascent.)\n",
    "\n",
    "This performs all the gradient computations for us! Isn't that great?\n",
    "\"\"\"\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(neg_LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we've setup the entire neural network graph, so let's start training it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "    \n",
    "xs, ys = data['train/image'], data['train/label'].reshape(11623,1)\n",
    "for epoch in range(training_epochs):\n",
    "    # Run optimization op (backprop) and neg_LL op (to get loss value)\n",
    "    _, nll = sess.run([optimizer, neg_LL], feed_dict={x: xs, y: ys}) \n",
    "    # following https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf\n",
    "    # Print cost\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"-LL =\", \"{:.9f}\".format(nll))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification accuracy\n",
    "Now, let's see how well our classifier does on the test dataset. To do this, we need to build an\n",
    "operation \"accuracy\", which, given a set of datapoints X and labels Y, computes the accuracy of classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifications = tf.round(prob_y1) # get classifications of each datapoint from probabilities\n",
    "accurracy = 1.0 - tf.reduce_mean(tf.abs(classifications - y)) # compute accuracy using classifications and truth y\n",
    "# training\n",
    "xs, ys = data['train/image'], data['train/label'].reshape(11623,1)\n",
    "# sess.run is how we use the tensorflow model to evaluate things.\n",
    "# feed_dict provides the input to the placeholders x and y, \n",
    "# and accurracy is what we want to evaluate.\n",
    "training_acc = sess.run(accurracy, feed_dict={x: xs, y: ys})\n",
    "# testing\n",
    "xs, ys = data['test/image'], data['test/label'].reshape(2115,1)\n",
    "testing_acc = sess.run(accurracy, feed_dict={x: xs, y: ys})\n",
    "print(\"Training accuracy: %f\" % training_acc)\n",
    "print(\"Testing accuracy: %f\" % testing_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see accuracy > 0.999 on the testing dataset! So this model does a very good job of classifying zeros and ones. Let's try to visualize approximately what it is doing in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Maximal Activation in the Context of MNIST\n",
    "Read question 2 in the section hand out and ponder it a bit before proceeding. This next portion deals with similar concepts in the context of the data we've already used above for question 1. Here, however, we're trying to generate, from scratch, a images that will maximally activate our final \"1\" neuron\n",
    "\n",
    "We can find the image that the model most classifies as a \"1\" by doing gradient ascent on prob_y1, with respect to the input image. We first initialize an image where all pixel values are 0. Then, repeatedly, we feed the image into the model, compute the gradients of prob_y1 with respect to the image, and then update the image with those gradients to increase prob_y1.\n",
    "\n",
    "Below is most of the code to complete this task on the MNIST dataset. There are two lines that you have to supply (the gradient update step) marked with TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trying to find the image that the model is most confident is a 1:\"\"\"\n",
    "# first, find gradient of probs with respect to image\n",
    "one_grads = tf.gradients(prob_y1, x)\n",
    "zero_grads = tf.gradients(prob_y0, x)\n",
    "\n",
    "# now, repeatedly feed an image into model, calculate gradients, and perform gradient ascent on prob_y1 with respect\n",
    "# to the image: \n",
    "eta = 1 # the learning rate; this choice is important here\n",
    "# these are our starting images that we will update with gradient ascent\n",
    "perfect_zero = np.zeros((1,28*28))\n",
    "perfect_one = np.zeros((1,28*28))\n",
    "\n",
    "for i in range(10000):\n",
    "    p0_grads = sess.run((zero_grads), feed_dict={x:perfect_zero})[0] # the gradient with respect to perfect_zero\n",
    "    p1_grads = sess.run((one_grads), feed_dict={x:perfect_one})[0] # the gradient with respect to perfect_one\n",
    "    \n",
    "    # TODO: YOU NEED TO ADD TWO LINES HERE to perform the gradient ascent update on perfect_zero and perfect_one\n",
    "\n",
    "    \n",
    "    # ----------------------\n",
    "\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, sharey=True, figsize=(5,2))\n",
    "# images that we've generated are not scaled like MNIST images. So need to rescale:\n",
    "def normalize_image(image):\n",
    "    # normalize so all pixels in image are between 0 and 1\n",
    "    image = image - np.min(image)\n",
    "    return image / np.max(image)\n",
    "    \n",
    "\n",
    "perfect_zero = normalize_image(perfect_zero)\n",
    "perfect_one = normalize_image(perfect_one)\n",
    "gen_image(perfect_zero, ax1)\n",
    "gen_image(perfect_one, ax2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What do you notice about the images above? Do they look like \"perfect\" ones or zeros? Why or why not?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try something else: instead of finding the image that maximally activates the probability of being a one or zero, let's find the images that maximally activate each of the two \"neurons\" in the hidden layer:\n",
    "\n",
    "YOUR TASK:\n",
    "the below code is mostly copy pasted from the above cell. Modify it so that you find the images that maximally activate the two neurons in the hidden layer (rather than the final probabilities). The lines you need to change are commented.\n",
    "\n",
    "HINT: The neurons in the hidden layer are represented as \"a\". \"a\" is a tensor with shape [batch_size, 2]. Tensors can be indexed the same way as numpy arrays. That is, if \"a\" is\n",
    "\n",
    "[[0,1],\n",
    " [0,1],\n",
    " [0,1],\n",
    " [2,3]]\n",
    " \n",
    "Then \"a[:,0]\" will give \n",
    "[0,0,0,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \"zero_grads\" should be the gradients of the first neuron in the hidden layer, with respect to the input image\n",
    "# and \"one_grads\" should be the gradients with respect to the second neuron in the hidden layer.\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "\n",
    "# now, repeatedly feed an image into model, calculate gradients, and perform gradient ascent with respect\n",
    "# to the image: \n",
    "eta = .01 # choice of learning rate (eta) is important here\n",
    "perfect_zero = np.zeros((1,28*28))\n",
    "perfect_one = np.zeros((1,28*28))\n",
    "\n",
    "for i in range(1000):\n",
    "    p0_grads = sess.run((zero_grads), feed_dict={x:perfect_zero})[0]\n",
    "    p1_grads = sess.run((one_grads), feed_dict={x:perfect_one})[0]\n",
    "    # TODO: update images with gradients (gradient ascent step)\n",
    "\n",
    "    \n",
    "    # ----------------------\n",
    "\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, sharey=True, figsize=(5,2))\n",
    "# images that we've generated are not scaled like mnist images. So need to rescale:\n",
    "def normalize_image(image):\n",
    "    # normalize so all pixels in image are between 0 and 1\n",
    "    image = image - np.min(image)\n",
    "    return image / np.max(image)\n",
    "    \n",
    "\n",
    "perfect_zero = normalize_image(perfect_zero)\n",
    "perfect_one = normalize_image(perfect_one)\n",
    "gen_image(perfect_zero, ax1)\n",
    "gen_image(perfect_one, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just done is a very simple start towards something really cool, which is neural networks \"dreaming\"\n",
    "up images of things that they know how to classify! In this example, the visualizations the hidden neurons are learning relatively simple things, but for more complicated datasets, hidden neurons can learn interesting things (like how to detect wheels, eyes, branches, etc (even if none of those are things that need to be classified)). Take a look at the blog post below for some awesome examples of this.\n",
    "\n",
    "Deep dream blog post:\n",
    "https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights of the last layer of the model\n",
    "print(sess.run(W_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
